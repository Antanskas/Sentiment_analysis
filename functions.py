# -*- coding: utf-8 -*-
"""functions.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_LDt6bQgUOXNgQsRkDm8SizJAaY9USNe
"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
import time
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV

"""### Data processing"""

def load_data(dataset_path):
  # reading files into dataframes
  pos = pd.read_csv(dataset_path + 'rt-polarity.pos', sep='\t', header=None, names=['Sentiment'])
  neg = pd.read_csv(dataset_path + 'rt-polarity.neg', sep='\t', header=None, names=['Sentiment'])
  pos['Target'] = 1
  neg['Target'] = 0

  # creating one X dataframe
  X_full = pd.concat((pos, neg), axis=0, ignore_index=True)
  X_full = shuffle(X_full)

  # splitting data into training and testing parts
  X_train, X_test, y_train, y_test = train_test_split(X_full['Sentiment'].values,
                                                      X_full['Target'].values,
                                                      random_state=0,
                                                      test_size=0.2)
  return X_full, X_train, X_test, y_train, y_test

# vectorize tokenized examples
def vectorizer(X_train, X_test):
  vect = TfidfVectorizer(use_idf=True)
  X_train_tfidf = vect.fit_transform(X_train)
  X_test_tfidf = vect.transform(X_test)
  return X_train_tfidf, X_test_tfidf

"""### Naive Bayes"""

# Calculate Naive Bayes algorithm parameters
def calculations(X, y):
  p = X[y==1].sum(0) + 1
  q = X[y==0].sum(0) + 1
  r = np.log((p/p.sum())/(q/q.sum()))
  b = np.log(len(p) / len(q))
  return r, b

# Predict targets of given matrix using learned parameters
def predict_Bayes(X, r, b):
  pre_preds = X @ r.T + b
  predictions= pre_preds.T > 0
  return predictions

# Calculate our predictions accuracy
def accuracy(y_true, predictions):
  accuracy = (predictions == y_true).mean()*100
  return accuracy

# grid search model to find optimal hyperparameters
def nb_model(alpha):
  grid={"alpha": alpha}
  nb_model = MultinomialNB()
  model = GridSearchCV(nb_model, grid,cv=5)
  return model

"""### Logistic regression"""

# grid search model to find optimal hyperparameters
def loregr_model(C_range, penalties):
  grid={"C": C_range, "penalty": penalties}
  logreg = LogisticRegression()
  model = GridSearchCV(logreg, grid,cv=2)
  return model

"""### Model evaluation"""

def evaluate_model(y_true, predictions):
  cm = confusion_matrix(y_true, predictions)
  print("\nConfusion matrix: \n", cm)
  print("")
  cr = classification_report(y_true, predictions)
  print("Classification report: \n", cr)
  print("")
  sns.heatmap(cm, annot=True)
  plt.show()